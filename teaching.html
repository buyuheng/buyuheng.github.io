<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>EEL 6935 Information-Theoretic Methods in Machine Learning (Spring 2023)</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Yuheng Bu</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="Yuheng_CV.pdf">CV</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="group.html">Group</a></div>
<div class="menu-item"><a href="teaching.html" class="current">Teaching</a></div>
<div class="menu-item"><a href="https://scholar.google.com/citations?user=1jPQEVMAAAAJ&hl=en">Google&nbsp;Scholar</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>EEL 6935 Information-Theoretic Methods in Machine Learning (Spring 2023)</h1>
</div>
<p>Department of <a href="https://www.ece.ufl.edu/">Electrical and Computer Engineering</a>, <a href="https://www.ufl.edu/">University of Florida</a>
<br />Lectures: 11:45 AM - 12:35 PM MWF, 0327 MAEA</p>
<h2>Contact Information </h2>
<p>Instructor: Dr. <a href="https://buyuheng.github.io/">Yuheng Bu</a><br />
Office: 453 New Engineering Building<br />
Email: buyuheng [at] ufl [dot] edu<br /> 
Office Hours: 1:30 PM - 2:30 PM F, 453 NEB</p>
<p><a href="https://www.ece.ufl.edu/wp-content/uploads/syllabi/Spring2023/EEL6935_Info_Theory_Mach_Learn_Bu_Spring_2023_final.pdf">[syllabus]</a> 
<a href="https://ufl.instructure.com/courses/473853">[canvas]</a> </p>
<h2>Outline </h2>
<p>The interplay between information theory and machine learning is a constant theme in the development of both fields. This course will discuss how techniques rooted in information theory play a key role in understanding the fundamental limits of statistical inference and supervised learning problems regarding generalization error and sample complexity. In particular, we will start from traditional statistical inference, which provides us with the basic intuition about the essence of information-theoretic methods, i.e., connecting mathematically defined information measures with operational meaning. Towards the end, we will also discuss the recent trend of using information-theoretic methods in characterizing the generalization error of learning algorithms.</p>
<p>Further descriptions of these areas are given in the <a href="https://www.ece.ufl.edu/wp-content/uploads/syllabi/Spring2023/EEL6935_Info_Theory_Mach_Learn_Bu_Spring_2023_final.pdf">syllabus</a>. </p>
<h2>General References</h2>
<p>There is no required textbook &ndash; we will provide a set of hand-written notes on <a href="https://ufl.instructure.com/courses/473853">canvas</a>. The notes contain pointers to relevant references. Some general references are given below.</p>
<ul>
<li><p>Joy A. Thomas and Thomas M. Cover. <a href="http://staff.ustc.edu.cn/~cgong821/Wiley.Interscience.Elements.of.Information.Theory.Jul.2006.eBook-DDU.pdf">&ldquo;Elements of Information Theory,&rdquo;</a> Wiley-Interscience, 2006</p>
</li>
<li><p>Y. Polyanskiy and Y. Wu. <a href="https://people.lids.mit.edu/yp/homepage/data/itbook-export.pdf">&ldquo;Information Theory: From Coding to Learning,&rdquo;</a> Cambridge University Press, 2023 (book draft)</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2024-08-25 23:12:14 Eastern Daylight Time, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
